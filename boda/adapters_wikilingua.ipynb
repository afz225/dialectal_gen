{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 15:57:36.955261: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-02 15:57:36.955468: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-02 15:57:37.038010: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-02 15:57:37.202850: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 15:57:38.713838: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/abdelrahman.sadallah/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from copy import copy\n",
    "from transformers import AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import os\n",
    "from math import inf\n",
    "from datasets import load_dataset,concatenate_datasets\n",
    "from transformers import AutoModelForMultipleChoice\n",
    "from adapters import init,AutoAdapterModel\n",
    "import numpy as np\n",
    "from transformers import TrainingArguments, EvalPrediction\n",
    "from adapters import AdapterTrainer\n",
    "import pandas as pd\n",
    "from adapters import AdapterSetup, AutoAdapterModel\n",
    "from evaluate import load\n",
    "from transformers import DataCollatorForSeq2Seq,RobertaForMultipleChoice,RobertaTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    max_input_length = 1024\n",
    "    max_target_length = 128\n",
    "    prefix = \"summarize: \"    \n",
    "    inputs = [prefix + doc for doc in examples[\"source\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    labels = tokenizer(text_target=examples[\"target\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "  \n",
    "def compute_metrics_dia_wikilingua(eval_preds):\n",
    "        print('hi')\n",
    "        print(\"eval_preds\",eval_preds)\n",
    "        predictions, labels = eval_preds\n",
    "\n",
    "    \n",
    "        # print(predictions)\n",
    "        # print(labels)\n",
    "    \n",
    "        if isinstance(predictions, tuple):\n",
    "            predictions = predictions[0]\n",
    "            \n",
    "\n",
    "        print(type(predictions))\n",
    "        # print(predictions.size())\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        # Rouge expects a newline after each sentence\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "        \n",
    "        # Note that other metrics may not have a `use_aggregator` parameter\n",
    "        # and thus will return a list, computing a metric for each sentence.\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "        # Extract a few results\n",
    "        result = {key: value * 100 for key, value in result.items()}\n",
    "        \n",
    "        # Add mean generated length\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "        result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "        \n",
    "        return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dialects = [\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\"]\n",
    "\n",
    "train_dialects =[\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\"]\n",
    "test_dialects =[\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\",'chi','iri','pak']\n",
    "\n",
    "# train_dialects= [ \"aus\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load(\"rouge\")\n",
    "\n",
    "model_name = 'google/flan-t5-base'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "init(model) \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for GEM/wiki_lingua contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/GEM/wiki_lingua\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d8ccdfeb5b43ed97ab8dd2c2c3f605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27489 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f24c69f4c64402aa8d6a376ccd09184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec431ccf33474ab39c93cbbf57554b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.34M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0a43d126a0c41bd8a77a20dfec41d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/272k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056518b2b68c4407b26ed9a3fc536ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/256k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b7d0679124477988f3a0e6c7f01b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1671d2dbd6ba44afb37f09fac614b09f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc480e10d201422c937fa8a697dc8535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3e163ed0504fd2945d812e118d856a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58490b1a0a574404a1ea912eaf1e6622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd4f162b33540d68686a3e48e73f285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931fd20fe29349b9aa0eaa9eb3e058e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dacfe32e771d451395f0a0cfa699a3b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/261k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1f80c42f0d46f3bb7882046ad2df0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/244k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96317ef1a2a84988b67177f10f4cb2b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3ef805321a4f2585a93bb5384185e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f82d5facd59d4dcc8bee710a257f1662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7748671520cb4fb693cf54551c6f1b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc56ae036144d98a29c21baeae5c591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aed5e296ee04619b4470e8a185988b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20dcd8f343349f5bfa868262f513031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.28M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2463c8ecc8ea43cea75106a3852da0cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/259k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4224d84d35ed4a0ea57695fc88ef42ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f0f6801b404d30a12dcdd3af0ef378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "172ad7762b444de38113aaaf22162a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca3d1169cb94edabf2bc953ca3fcc05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb1c098607b4ab880a417d09b0822ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b8f094df744ac69b8441255a85eb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f266ab9db18649d28d47ab58dcd654da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf22c5aee8345f5a8f92da6542f2192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bf119db30c44159941de09e2daa5376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/288k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1ea8d7936742d2ac84cab4ed20ba6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/272k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c72f5915529437c9d7f3b231e6025fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a172b4f6a9974ff5ad10c7d791535037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d95eae5b57984eb6a536685dda274f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0ad652f768245089ca2a5ffbe1ad545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b554fa2dd164626b93ffb2ee8d0bb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d182fe0e880c4666a30af97e283f92cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e534985e35463ebbdc4bfdff7082ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d63b1e5be9044d58615b6f24d163b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c472a7fcf884ab398257341f07d4bd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/254k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f52d6a0d49492ebeeb638b003991a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f92f4373b94260b56f8bb2055a0dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0edc8a1ccde4dbb88f40f63aa6122da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7306a1fa0b94ff6b2dbadb81e5a7e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e19f6897774777aa5f093b2b9840d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89358407c41e4df98ca2494dbc9e0fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########33 Loading DS1\n",
    "\n",
    "DS = {}\n",
    "\n",
    "for dialect in new_dialects:\n",
    "\n",
    "    if dialect != \"std-dia\":\n",
    "        ds = load_dataset(\"ashabrawy/dia_wikilingua\", dialect).remove_columns(['Unnamed: 0'])\n",
    "        # columns = copy(ds['train'].column_names)\n",
    "        # # columns.remove('label')\n",
    "        # columns.remove('labels')\n",
    "        ds = ds.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "    else:\n",
    "        ds = load_dataset('GEM/wiki_lingua')\n",
    "\n",
    "        ## Here the dataset reference has type list, so we need to select the first element, to be able to concat it with dialects dataset \n",
    "        ds['train'] = ds['train'].select(range(500)).map(lambda example: {\"references\": example[\"references\"][0]})\n",
    "        ds['validation'] = ds['validation'].select(range(500)).map(lambda example: {\"references\": example[\"references\"][0]})\n",
    "        ds = ds.map(preprocess_function, batched=True)\n",
    "        \n",
    "\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    DS[dialect] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6871bdcdfb94076ba41619e74168f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00fc251f887c46708e810c23555194df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/273k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfdf4ec126c640afa13a687ca5dfe999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c5b51ebcd04693a400f78d5bb2275d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e205f93c36064ce0ab919b39b965d025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/273k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad75b6be69c441bbb96b140178988fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e46ca1d09ae4ed5b11870ea415bc606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "for dialect in ['iri','chi','pak']:\n",
    "    ds = load_dataset(\"ashabrawy/dia_wikilingua\", dialect)\n",
    "    ds = ds.map(preprocess_function, batched=True)\n",
    "    ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    DS[dialect] = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DS.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,d in DS.items():\n",
    "\n",
    "    # if ['gem_id', 'gem_parent_id'] in DS[k]['train'].column_names:\n",
    "    #     DS[k]['train'] = DS[k]['train'].remove_columns(['gem_id', 'gem_parent_id'])\n",
    "    # if ['gem_id', 'gem_parent_id'] in DS[k]['validation'].column_names:\n",
    "    \n",
    "    #     DS[k][''] = DS[k]['validation'].remove_columns(['gem_id', 'gem_parent_id'])\n",
    "\n",
    "\n",
    "    if k in ['iri','chi','pak']:\n",
    "        DS[k]['validation'] = DS[k]['test'].select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "    else:\n",
    "        DS[k]['validation'] = DS[k]['validation'].select_columns(['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "for k,d in DS.items():\n",
    "    print( ['gem_id', 'gem_parent_id'] in  DS[k]['train'].column_names)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'std-dia': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 27489\n",
       "     })\n",
       "     sampled_validation: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 3000\n",
       "     })\n",
       "     sampled_test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 3000\n",
       "     })\n",
       " }),\n",
       " 'aus': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'hon': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'nig': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'col': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'wel': DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 500\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'iri': DatasetDict({\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'references', 'source', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'chi': DatasetDict({\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'references', 'source', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " }),\n",
       " 'pak': DatasetDict({\n",
       "     test: Dataset({\n",
       "         features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'references', 'source', 'target', 'input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       "     validation: Dataset({\n",
       "         features: ['input_ids', 'attention_mask', 'labels'],\n",
       "         num_rows: 100\n",
       "     })\n",
       " })}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_ds = [y['train'] for x,y in DS.items() if x in train_dialects]\n",
    "\n",
    "combined_ds = concatenate_datasets(combined_ds).shuffle().select(range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['gem_id', 'gem_parent_id', 'source_language', 'target_language', 'source', 'target', 'references', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# for dialect in train_dialects:\n",
    "\n",
    "\n",
    "#     torch.cuda.empty_cache()\n",
    "#     model.add_adapter(dialect, config=\"seq_bn\", overwrite_ok=True)\n",
    "#     model.train_adapter(dialect)\n",
    "\n",
    "#     training_args = Seq2SeqTrainingArguments(\n",
    "#         learning_rate=1e-4,\n",
    "#         num_train_epochs=2,\n",
    "#         per_device_train_batch_size=8,\n",
    "#         per_device_eval_batch_size=8,\n",
    "#         logging_steps=200,\n",
    "#         output_dir=f\"/l/users/abdelrahman.sadallah/dialectal_gen/wiki_lingua-{dialect}\",\n",
    "#         overwrite_output_dir=True,\n",
    "#         # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "#         remove_unused_columns=False,\n",
    "#         predict_with_generate = True\n",
    "#     )\n",
    "#     trainer = Seq2SeqTrainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=DS[dialect][\"train\"] ,#  .select(range(10)),\n",
    "#         eval_dataset=DS[dialect][\"validation\"].select(range(100)),\n",
    "#         compute_metrics=compute_metrics_dia_wikilingua,\n",
    "#         data_collator= DataCollatorForSeq2Seq(tokenizer, model=model,label_pad_token_id=-100,pad_to_multiple_of=8)\n",
    "#     )\n",
    "#     trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "#     model.save_adapter(f\"/l/users/abdelrahman.sadallah/dialectal_gen/wiki_lingua-adapter-{dialect}\", dialect, with_head=False)\n",
    "\n",
    "\n",
    "#     DS[dialect].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "#     if dialect == \"std-dia\":\n",
    "#         for test_dialect in test_dialects:\n",
    "#             DS[test_dialect].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "            \n",
    "#             eval_metrics = trainer.evaluate(DS[test_dialect][\"validation\"].select(range(100)))\n",
    "#             eval_metrics['test_set'] = test_dialect\n",
    "#             eval_metrics['train_set'] = dialect\n",
    "\n",
    "#             results = pd.DataFrame(eval_metrics, index=[0])\n",
    "\n",
    "#             print(results)\n",
    "#             results.to_csv(f\"train-{dialect}test-{test_dialect}-wikilingua.csv\")\n",
    "#     else:\n",
    "#             test_dialect = dialect\n",
    "#             DS[test_dialect].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "            \n",
    "#             eval_metrics = trainer.evaluate(DS[test_dialect][\"validation\"].select(range(100)))\n",
    "#             eval_metrics['test_set'] = test_dialect\n",
    "#             eval_metrics['train_set'] = dialect\n",
    "\n",
    "#             results = pd.DataFrame(eval_metrics, index=[0])\n",
    "\n",
    "#             print(results)\n",
    "#             results.to_csv(f\"train-{dialect}test-{test_dialect}-wikilingua.csv\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "   # fusuion\n",
    "\n",
    "from adapters import T5AdapterModel\n",
    "from adapters.composition import Fuse\n",
    "\n",
    "model_name = 'google/flan-t5-base'\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# # init(model) \n",
    "model = T5AdapterModel.from_pretrained(model_name, )\n",
    "\n",
    "for dialect in train_dialects:\n",
    "    model.load_adapter(f\"/l/users/abdelrahman.sadallah/dialectal_gen/wiki_lingua-adapter-{dialect}\", with_head=False)\n",
    "\n",
    "\n",
    "\n",
    "model.add_adapter_fusion(Fuse(\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\"))\n",
    "model.set_active_adapters(Fuse(\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\"))\n",
    "model.train_adapter_fusion(Fuse(\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\")) \n",
    "\n",
    "\n",
    "3################################## old that doesn't;t work ######################333\n",
    "# model.set_active_adapters(train_dialects)\n",
    "# model.add_adapter_fusion(train_dialects)\n",
    "# # model.train_adapter_fusion(train_dialects) \n",
    "\n",
    "# # torch.cuda.empty_cache()\n",
    "# model.add_adapter_fusion(train_dialects, \"dynamic\")\n",
    "# model.train_adapter_fusion(train_dialects) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5AdapterModel(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5StackWithAdapters(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (invertible_adapters): ModuleDict()\n",
       "    )\n",
       "    (decoder): T5StackWithAdapters(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerCrossAttentionWithAdapters(\n",
       "              (EncDecAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (2): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerCrossAttentionWithAdapters(\n",
       "              (EncDecAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (2): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (default): Seq2SeqLMHead(\n",
       "      (0): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.204, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbodasadallah2\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abdelrahman.sadallah/mbzuai/dialectal_gen/boda/wandb/run-20240502_161448-6znjuohp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bodasadallah2/huggingface/runs/6znjuohp' target=\"_blank\">ruby-frost-258</a></strong> to <a href='https://wandb.ai/bodasadallah2/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bodasadallah2/huggingface' target=\"_blank\">https://wandb.ai/bodasadallah2/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bodasadallah2/huggingface/runs/6znjuohp' target=\"_blank\">https://wandb.ai/bodasadallah2/huggingface/runs/6znjuohp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 07:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.424800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.220000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=630, training_loss=2.2982847910078745, metrics={'train_runtime': 457.2652, 'train_samples_per_second': 10.935, 'train_steps_per_second': 1.378, 'total_flos': 7171526838730752.0, 'train_loss': 2.2982847910078745, 'epoch': 10.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=200,\n",
    "    output_dir=f\"/l/users/abdelrahman.sadallah/dialectal_gen/wiki_lingua-fusion\",\n",
    "    overwrite_output_dir=True,\n",
    "    # The next line is important to ensure the dataset labels are properly passed to the model\n",
    "    remove_unused_columns=False,\n",
    "    predict_with_generate = True,\n",
    "    # safe_serialization=False\n",
    "\n",
    ")\n",
    "# trainer = AdapterTrainer(\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_ds ,#  .select(range(10)),\n",
    "    eval_dataset=DS[\"std-dia\"][\"validation\"].select(range(100)),\n",
    "    compute_metrics=compute_metrics_dia_wikilingua,\n",
    "    data_collator= DataCollatorForSeq2Seq(tokenizer, model=model,label_pad_token_id=-100,pad_to_multiple_of=8)\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.sadallah/mambaforge/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1156: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 02:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506caa17b50>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c983fa00>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c983f910>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c983fe20>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c81348e0>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c8134a00>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c81354b0>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c90bb9a0>\n",
      "<class 'numpy.ndarray'>\n",
      "hi\n",
      "eval_preds <transformers.trainer_utils.EvalPrediction object at 0x1506c90b94e0>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model.save_adapter_fusion(f\"/l/users/abdelrahman.sadallah/dialectal_gen/wiki_lingua-adapter-fusion\", [\"std-dia\", \"aus\", \"hon\", \"nig\", \"col\", \"wel\"], with_head=False)\n",
    "\n",
    "\n",
    "rr = []\n",
    "for test_dialect in test_dialects:\n",
    "    DS[test_dialect].set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "\n",
    "    \n",
    "    eval_metrics = trainer.evaluate(DS[test_dialect][\"validation\"].select(range(100)))\n",
    "    eval_metrics['test_set'] = test_dialect\n",
    "    eval_metrics['train_set'] = 'combined_dataset'\n",
    "\n",
    "    rr.append(eval_metrics)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eval_loss': 1.8809133768081665,\n",
       "  'eval_rouge1': 29.7665,\n",
       "  'eval_rouge2': 12.3183,\n",
       "  'eval_rougeL': 26.1264,\n",
       "  'eval_rougeLsum': 28.8989,\n",
       "  'eval_gen_len': 18.66,\n",
       "  'eval_runtime': 15.2237,\n",
       "  'eval_samples_per_second': 6.569,\n",
       "  'eval_steps_per_second': 0.854,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'std-dia',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.1812870502471924,\n",
       "  'eval_rouge1': 31.3851,\n",
       "  'eval_rouge2': 12.2592,\n",
       "  'eval_rougeL': 27.4832,\n",
       "  'eval_rougeLsum': 30.4854,\n",
       "  'eval_gen_len': 18.95,\n",
       "  'eval_runtime': 15.3533,\n",
       "  'eval_samples_per_second': 6.513,\n",
       "  'eval_steps_per_second': 0.847,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'aus',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.7965574264526367,\n",
       "  'eval_rouge1': 26.4739,\n",
       "  'eval_rouge2': 7.997,\n",
       "  'eval_rougeL': 22.9539,\n",
       "  'eval_rougeLsum': 25.6177,\n",
       "  'eval_gen_len': 18.46,\n",
       "  'eval_runtime': 14.9752,\n",
       "  'eval_samples_per_second': 6.678,\n",
       "  'eval_steps_per_second': 0.868,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'hon',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.3684868812561035,\n",
       "  'eval_rouge1': 30.0533,\n",
       "  'eval_rouge2': 10.4181,\n",
       "  'eval_rougeL': 26.5662,\n",
       "  'eval_rougeLsum': 29.0674,\n",
       "  'eval_gen_len': 18.86,\n",
       "  'eval_runtime': 15.5188,\n",
       "  'eval_samples_per_second': 6.444,\n",
       "  'eval_steps_per_second': 0.838,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'nig',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.122570514678955,\n",
       "  'eval_rouge1': 29.383,\n",
       "  'eval_rouge2': 10.55,\n",
       "  'eval_rougeL': 24.9296,\n",
       "  'eval_rougeLsum': 28.2667,\n",
       "  'eval_gen_len': 18.96,\n",
       "  'eval_runtime': 15.7106,\n",
       "  'eval_samples_per_second': 6.365,\n",
       "  'eval_steps_per_second': 0.827,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'col',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.481393337249756,\n",
       "  'eval_rouge1': 28.6504,\n",
       "  'eval_rouge2': 8.4752,\n",
       "  'eval_rougeL': 24.8358,\n",
       "  'eval_rougeLsum': 27.6161,\n",
       "  'eval_gen_len': 18.8,\n",
       "  'eval_runtime': 15.082,\n",
       "  'eval_samples_per_second': 6.63,\n",
       "  'eval_steps_per_second': 0.862,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'wel',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.023733377456665,\n",
       "  'eval_rouge1': 28.1166,\n",
       "  'eval_rouge2': 10.1366,\n",
       "  'eval_rougeL': 23.5628,\n",
       "  'eval_rougeLsum': 26.9295,\n",
       "  'eval_gen_len': 18.62,\n",
       "  'eval_runtime': 15.5324,\n",
       "  'eval_samples_per_second': 6.438,\n",
       "  'eval_steps_per_second': 0.837,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'chi',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.3420839309692383,\n",
       "  'eval_rouge1': 27.0826,\n",
       "  'eval_rouge2': 9.0722,\n",
       "  'eval_rougeL': 23.5643,\n",
       "  'eval_rougeLsum': 26.3302,\n",
       "  'eval_gen_len': 18.62,\n",
       "  'eval_runtime': 15.5421,\n",
       "  'eval_samples_per_second': 6.434,\n",
       "  'eval_steps_per_second': 0.836,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'iri',\n",
       "  'train_set': 'combined_dataset'},\n",
       " {'eval_loss': 2.411780834197998,\n",
       "  'eval_rouge1': 25.7012,\n",
       "  'eval_rouge2': 7.2278,\n",
       "  'eval_rougeL': 21.2734,\n",
       "  'eval_rougeLsum': 24.4264,\n",
       "  'eval_gen_len': 18.5,\n",
       "  'eval_runtime': 15.7657,\n",
       "  'eval_samples_per_second': 6.343,\n",
       "  'eval_steps_per_second': 0.825,\n",
       "  'epoch': 10.0,\n",
       "  'test_set': 'pak',\n",
       "  'train_set': 'combined_dataset'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   eval_loss  eval_rouge1  eval_rouge2  eval_rougeL  eval_rougeLsum  \\\n",
      "0   1.880913      29.7665      12.3183      26.1264         28.8989   \n",
      "1   2.181287      31.3851      12.2592      27.4832         30.4854   \n",
      "2   2.796557      26.4739       7.9970      22.9539         25.6177   \n",
      "3   2.368487      30.0533      10.4181      26.5662         29.0674   \n",
      "4   2.122571      29.3830      10.5500      24.9296         28.2667   \n",
      "5   2.481393      28.6504       8.4752      24.8358         27.6161   \n",
      "6   2.023733      28.1166      10.1366      23.5628         26.9295   \n",
      "7   2.342084      27.0826       9.0722      23.5643         26.3302   \n",
      "8   2.411781      25.7012       7.2278      21.2734         24.4264   \n",
      "\n",
      "   eval_gen_len  eval_runtime  eval_samples_per_second  eval_steps_per_second  \\\n",
      "0         18.66       15.2237                    6.569                  0.854   \n",
      "1         18.95       15.3533                    6.513                  0.847   \n",
      "2         18.46       14.9752                    6.678                  0.868   \n",
      "3         18.86       15.5188                    6.444                  0.838   \n",
      "4         18.96       15.7106                    6.365                  0.827   \n",
      "5         18.80       15.0820                    6.630                  0.862   \n",
      "6         18.62       15.5324                    6.438                  0.837   \n",
      "7         18.62       15.5421                    6.434                  0.836   \n",
      "8         18.50       15.7657                    6.343                  0.825   \n",
      "\n",
      "   epoch test_set         train_set  \n",
      "0   10.0  std-dia  combined_dataset  \n",
      "1   10.0      aus  combined_dataset  \n",
      "2   10.0      hon  combined_dataset  \n",
      "3   10.0      nig  combined_dataset  \n",
      "4   10.0      col  combined_dataset  \n",
      "5   10.0      wel  combined_dataset  \n",
      "6   10.0      chi  combined_dataset  \n",
      "7   10.0      iri  combined_dataset  \n",
      "8   10.0      pak  combined_dataset  \n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(rr)\n",
    "# results = pd.DataFrame.from_records(rr)\n",
    "print(results)\n",
    "results.to_csv(f\"train-combined-test-{test_dialect}-wikilingua_fusion.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5AdapterModel(\n",
       "  (transformer): T5Model(\n",
       "    (shared): Embedding(32128, 768)\n",
       "    (encoder): T5StackWithAdapters(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (invertible_adapters): ModuleDict()\n",
       "    )\n",
       "    (decoder): T5StackWithAdapters(\n",
       "      (embed_tokens): Embedding(32128, 768)\n",
       "      (block): ModuleList(\n",
       "        (0): T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (relative_attention_bias): Embedding(32, 12)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerCrossAttentionWithAdapters(\n",
       "              (EncDecAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (2): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1-11): 11 x T5Block(\n",
       "          (layer): ModuleList(\n",
       "            (0): T5LayerSelfAttentionWithAdapters(\n",
       "              (SelfAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (1): T5LayerCrossAttentionWithAdapters(\n",
       "              (EncDecAttention): T5AttentionWithAdapters(\n",
       "                (q): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (k): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (v): LoRALinearTorch(\n",
       "                  in_features=768, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "                (prefix_tuning): PrefixTuningLayer(\n",
       "                  (prefix_gates): ModuleDict()\n",
       "                  (pool): PrefixTuningPool(\n",
       "                    (prefix_tunings): ModuleDict()\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict()\n",
       "              (adapter_fusion_layer): ModuleDict()\n",
       "            )\n",
       "            (2): T5LayerFFWithAdapters(\n",
       "              (DenseReluDense): T5DenseGatedActDense(\n",
       "                (wi_0): Linear(in_features=768, out_features=2048, bias=False)\n",
       "                (wi_1): LoRALinearTorch(\n",
       "                  in_features=768, out_features=2048, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (wo): LoRALinearTorch(\n",
       "                  in_features=2048, out_features=768, bias=False\n",
       "                  (loras): ModuleDict()\n",
       "                )\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (act): NewGELUActivation()\n",
       "              )\n",
       "              (layer_norm): T5LayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (adapters): ModuleDict(\n",
       "                (std-dia): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (aus): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (hon): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (nig): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (col): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (wel): Adapter(\n",
       "                  (non_linearity): Activation_Function_Class(\n",
       "                    (f): ReLU()\n",
       "                  )\n",
       "                  (adapter_down): Sequential(\n",
       "                    (0): Linear(in_features=768, out_features=48, bias=True)\n",
       "                    (1): Activation_Function_Class(\n",
       "                      (f): ReLU()\n",
       "                    )\n",
       "                  )\n",
       "                  (adapter_up): Linear(in_features=48, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (adapter_fusion_layer): ModuleDict(\n",
       "                (std-dia,aus,hon,nig,col,wel): BertFusion(\n",
       "                  (dropout): Dropout(p=0.1, inplace=False)\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): T5LayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (shared_parameters): ModuleDict()\n",
       "    (prefix_tuning): PrefixTuningPool(\n",
       "      (prefix_tunings): ModuleDict()\n",
       "    )\n",
       "  )\n",
       "  (heads): ModuleDict(\n",
       "    (default): Seq2SeqLMHead(\n",
       "      (0): Linear(in_features=768, out_features=32128, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
